{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Quality in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Debt in Code\n",
    "* Recall that in a DevOps-oriented organization, we want to reduce or remove **technical debt**\n",
    "* In practice, **technical debt** may mean poor quality or \"ugly\" code which is brittle (breaks easily) or hard to understand and maintain.\n",
    "* By following standards and best practices, and using automation, we can improve code quality and avoid technical debt\n",
    "* Just like camping - \"leave everything better than you found it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEP 8 - Python Code Style Guide\n",
    "- https://www.python.org/dev/peps/pep-0008/\n",
    "- Or, https://pep8.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But checking code style by hand is silly, so...\n",
    "- pep8 (the tool):  https://pep8.readthedocs.io/en/release-1.7.x/\n",
    "- PyLint:  https://www.pylint.org/\n",
    "- Flake8:  https://flake8.pycqa.org/en/latest/\n",
    "- Black:  https://black.readthedocs.io/en/stable/\n",
    "- Most of these are available as tools directly integrated into editors and IDEs, too!\n",
    "- Later, we'll incorporate these into our automated build / deploy pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Testing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "* Why Automated Testing?\n",
    "* Types of Automated Tests\n",
    "* Unit Testing in Python\n",
    "* Mocking and Faking\n",
    "* Test-Driven Development (TDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicing coding will make you a better coder\n",
    "## You'll write more efficient, more maintainable code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But how do you know your code actually works as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizzbuzz(num):\n",
    "    \"\"\"Returns 'fizz' for multiples of 3,\n",
    "       'buzz' for multiples of 5,\n",
    "       'fizzbuzz' for multiples of both,\n",
    "       or returns the number passed.\"\"\"\n",
    "    result = ''\n",
    "    if num % 3 == 0:\n",
    "        result += 'fizz'\n",
    "    if num % 5 == 0:\n",
    "        result += 'buzz'\n",
    "    result = result or num\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Until you execute the code, you really don't know it works...\n",
    "## So it's safe to assume it doesn't!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "* Verifies our code works as expected\n",
    "* Reproduces bugs that have been reported\n",
    "* Protects us against regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Testing\n",
    "* Just run the code by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fizzbuzz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fizzbuzz(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fizzbuzz(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fizzbuzz(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But we have to do that by hand, which takes time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Testing\n",
    "* Testing code with code\n",
    "* Fast\n",
    "* Repeatable\n",
    "* Deterministic (when built correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n",
      "Passed\n",
      "Passed\n",
      "Passed\n"
     ]
    }
   ],
   "source": [
    "def test_fizzbuzz_three():\n",
    "    result = fizzbuzz(3)\n",
    "    return result == 'fizz'\n",
    "\n",
    "def test_fizzbuzz_five():\n",
    "    result = fizzbuzz(5)\n",
    "    return result == 'buzz'\n",
    "\n",
    "def test_fizzbuzz_fifteen():\n",
    "    result = fizzbuzz(15)\n",
    "    return result == 'fizzbuzz'\n",
    "\n",
    "def test_fizzbuzz_two():\n",
    "    result = fizzbuzz(2)\n",
    "    return result == 2\n",
    "\n",
    "tests = [test_fizzbuzz_three, test_fizzbuzz_five, test_fizzbuzz_fifteen, test_fizzbuzz_two]\n",
    "for test in tests:\n",
    "    if test():\n",
    "        print('Passed')\n",
    "    else:\n",
    "        print('Failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a Test\n",
    "* **Setup** - anything the test needs in advance (i.e. test data, account creation)\n",
    "* **Execute** - actually run the code you're testing (the \"Code under Test\")\n",
    "* **Validation** - make sure the results match your expectations\n",
    "* **Teardown** - undo things in the setup phase to start with a \"clean slate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Automated Tests\n",
    "* **Unit Tests** - test small, isolated pieces\n",
    "* **Integration Tests** - test several pieces interoperating\n",
    "* **Acceptance Tests** - end to end tests at a high level against original requirements\n",
    "* **Smoke Tests** - quick tests after a deployment to ensure deployments succeeded\n",
    "* **Stress Tests** - performance tests to determine the limits of capabilities and plan resources\n",
    "* **Regression Tests** - unit tests specifically designed to reproduce a bug we've seen before, to ensure our fix stays fixed (i.e. doesn't regress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Unit Testing\n",
    "* the smallest testable parts of an application, called _units_, are individually and independently scrutinized to ensure they work\n",
    "* your functions/methods/procedures should do ONE thing (and do it well)â€“testing that thing should be relatively easy to explain\n",
    "* exercise the __!#@%@!$#__ out of the unit to be sure it works, especially with corner cases, not just the expected cases\n",
    "* sometimes called \"white box testing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest - Unit testing framework for Python\n",
    "* Test Discovery - just prefix your test modules and functions with `test_`\n",
    "* Test Validation - use Python keyword `assert` to test results\n",
    "* Summarizes results, displays errors\n",
    "* Lots of plugins (pytest-mocks, pytest-coverage, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In module test_fizzbuzz.py\n",
    "# from fizzbuzz import fizzbuzz\n",
    "def test_fizzbuzz_three():\n",
    "    result = fizzbuzz(3)\n",
    "    assert result == 'fizz'\n",
    "\n",
    "def test_fizzbuzz_five():\n",
    "    result = fizzbuzz(5)\n",
    "    assert result == 'buzz'\n",
    "\n",
    "def test_fizzbuzz_fifteen():\n",
    "    result = fizzbuzz(15)\n",
    "    assert result == 'fizzbuzz'\n",
    "\n",
    "def test_fizzbuzz_two():\n",
    "    result = fizzbuzz(2)\n",
    "    assert result == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /home/jr/iea-cohort-07/07_unit_testing\n",
      "plugins: mock-3.5.1, cov-2.8.1\n",
      "collected 8 items / 2 errors / 6 selected                                      \u001b[0m\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m________________ ERROR collecting demos/tests/test_fizzbuzz.py _________________\u001b[0m\n",
      "\u001b[31mImportError while importing test module '/home/jr/iea-cohort-07/07_unit_testing/demos/tests/test_fizzbuzz.py'.\n",
      "Hint: make sure your test modules/packages have valid Python names.\n",
      "Traceback:\n",
      "/usr/lib64/python3.7/importlib/__init__.py:127: in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "demos/tests/test_fizzbuzz.py:1: in <module>\n",
      "    from fizzbuzz import fizzbuzz\n",
      "E   ModuleNotFoundError: No module named 'fizzbuzz'\u001b[0m\n",
      "\u001b[31m\u001b[1m_____________ ERROR collecting demos/tests/test_fizzbuzz_types.py ______________\u001b[0m\n",
      "\u001b[31mImportError while importing test module '/home/jr/iea-cohort-07/07_unit_testing/demos/tests/test_fizzbuzz_types.py'.\n",
      "Hint: make sure your test modules/packages have valid Python names.\n",
      "Traceback:\n",
      "/usr/lib64/python3.7/importlib/__init__.py:127: in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "demos/tests/test_fizzbuzz_types.py:1: in <module>\n",
      "    from fizzbuzz import fizzbuzz\n",
      "E   ModuleNotFoundError: No module named 'fizzbuzz'\u001b[0m\n",
      "=========================== short test summary info ============================\n",
      "ERROR demos/tests/test_fizzbuzz.py\n",
      "ERROR demos/tests/test_fizzbuzz_types.py\n",
      "!!!!!!!!!!!!!!!!!!! Interrupted: 2 errors during collection !!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m2 errors\u001b[0m\u001b[31m in 0.16s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab:  Pytest Basics\n",
    "\n",
    "* Copy the `fizzbuzz` function into a module `fizzbuzz.py` and the tests into `test_fizzbuzz.py` and run Pytest.  Don't forget to `import fizzbuzz` in your test module!  Modify the code until all the tests are passing when you run pytest.\n",
    "* Add a few additional tests - try to think of some important cases we did not already test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What should be tested?\n",
    "* Code under test should be isolated\n",
    "* Test every code path possible\n",
    "* Avoid testing third party tools or frameworks (already tested)\n",
    "* Avoid dependencies (network, fileystem, environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Coverage\n",
    "* Measures how many lines of code were or were not executed during testing\n",
    "* Usually expressed as a percentage, i.e. \"77% code coverage\"\n",
    "* Ideally we'd have 100% code coverage, but...\n",
    "* Diminishing returns with boilerplate code, autogenerated code, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Code with Confidence\n",
    "- In addition to verifying the correctness of our code, good code coverage means we can fearlessly make changes (new features, refactoring) because we have the \"safety net\" of automated tests to ensure we didn't break anything in the process\n",
    "- If the whole team works on the same set of tests, we can collaborate much more simply without stepping on toes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Dependencies\n",
    "* often, our code interacts with environmental services, i.e., those which have undesirable side effects\n",
    " * inserting into database\n",
    " * posting on the web\n",
    " * system calls / interact with OS\n",
    "* â€¦as a developer, you care more that your code correctly called the system function for ejecting a CD rather than experiencing the CD tray open every time a test is run\n",
    "* and we want our tests to be deterministic, and isolated from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mocking\n",
    "* to deal with these kinds of services, we can use the __`mock`__ subpackage of the __`unitttest`__ library\n",
    "* included as of Python 3.3â€¦before that you need to download it via PyPI\n",
    "* a mock object is one that is substituted for a real object in a test case\n",
    "* unlike ordinary unit tests that assert on the state of an object, mock objects are used to test that interactions between multiple objects occurs as they should\n",
    "* writing test cases with mocks make our tests smarter, faster, and able to reveal more about how the software actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "somemethod() returned Hello World!\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import Mock\n",
    "\n",
    "mymock = Mock()\n",
    "mymock.somemethod.return_value = 'Hello World!'\n",
    "\n",
    "value = mymock.somemethod()\n",
    "print('somemethod() returned', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "somemethod() was called\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import Mock\n",
    "\n",
    "mymock = Mock()\n",
    "mymock.somemethod()\n",
    "\n",
    "try:\n",
    "    mymock.somemethod.assert_called()\n",
    "    print('somemethod() was called')\n",
    "except AssertionError:\n",
    "    print('somemethod() was not called!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def backup_file():\n",
    "    os.rename('datafile.csv', 'datafile.csv.bak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datafile.csv' -> 'datafile.csv.bak'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-40c7de2ed404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackup_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-49d767d23a7c>\u001b[0m in \u001b[0;36mbackup_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbackup_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datafile.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datafile.csv.bak'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datafile.csv' -> 'datafile.csv.bak'"
     ]
    }
   ],
   "source": [
    "backup_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backup_file() correctly called rename!\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "with patch('os.rename') as rename:\n",
    "    backup_file()\n",
    "    rename.assert_called_with('datafile.csv', 'datafile.csv.bak')\n",
    "    print('backup_file() correctly called rename!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In module funny.py\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_joke():\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "    response = requests.get('https://icanhazdadjoke.com/', headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        joke_id = None\n",
    "        joke = ''\n",
    "    else:\n",
    "        data = response.json()\n",
    "        joke_id = data.get('id')\n",
    "        joke = data.get('joke', 'Not Available')\n",
    "    return joke_id, joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s49h3ElVnrc',\n",
       " 'I burned 2000 calories today, I left my food in the oven for too long.')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_joke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In module test_funny.py\n",
    "from unittest.mock import patch\n",
    "\n",
    "\n",
    "def test_get_joke_success():\n",
    "    with patch('requests.get') as mock_get:\n",
    "        mock_response = mock_get.return_value\n",
    "        mock_response.status_code = 200\n",
    "        mock_response.json.return_value = {\n",
    "            'id': 12345,\n",
    "            'joke': 'This is a test joke. Wakka wakka!',\n",
    "            'status': 200\n",
    "        }\n",
    "        \n",
    "        joke_id, joke = get_joke()\n",
    "        assert joke_id == 12345\n",
    "        assert joke == 'This is a test joke. Wakka wakka!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_get_joke_success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_joke_server_error():\n",
    "    with patch('requests.get') as mock_get:\n",
    "        mock_response = mock_get.return_value\n",
    "        mock_response.status_code = 500\n",
    "        mock_response.json.return_value = {}\n",
    "        \n",
    "        joke_id, joke = get_joke()\n",
    "        assert joke_id is None\n",
    "        assert joke == 'Not Available'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-4809abf7be5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_get_joke_server_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-3ecfcbd0a843>\u001b[0m in \u001b[0;36mtest_get_joke_server_error\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mjoke_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoke\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_joke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mjoke_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mjoke\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Not Available'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_get_joke_server_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab:  Mocking\n",
    "\n",
    "Write a function that reads a string from an environment variable called **DATABASE_HOST** (see `os.getenv` or `os.environ`) and returns that string, prepended with the string **mysql://**.  Using the `unittest.mock` module, write unit tests for your function.  Try to cover all of the important cases you can think of - normal \"happy\" path, path when the variable doesn't exist, path when the variable does exist but is empty, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Test-Driven Development\n",
    "* TDD is a way of developing software that looks like this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![TDD](images/TDDflowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# TDD is NOT REALLY ABOUT TESTING!\n",
    "* traditionally, unit testing is about writing tests to verify the code worksâ€¦\n",
    "* â€¦whereas main focus of TDD is not about testing\n",
    "* writing a test before the code is implemented changes the way we think when we implement functionality\n",
    " * resulting code is more testable\n",
    " * usually simple, elegant design\n",
    " * easier to read and maintain\n",
    " * why?\n",
    "* so really about writing better code, and we get an automated test suite as a nice side effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lab: Test-Driven Development\n",
    "\n",
    "Using a TDD approach, write tests and then minimal implementation to pass the tests for a function called `find_primes` that takes a single argument - the maximum number to check for primes.  Try to remember to utilize the Red -> Green -> Refactor method and take small steps at first - you will get faster as you practice this approach more often!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
